{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a2aa18",
   "metadata": {},
   "source": [
    "# 强化学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe2d58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "\n",
    "class BotEnv(object):\n",
    "    def __init__(self):\n",
    "        self.n_actions = 3 # 三个动作：前进、左转、右转\n",
    "        self.power = 0 # 机器人电量\n",
    "        self.map = None # 当前房间网格地图\n",
    "        self.pos = None # 机器人当前位置\n",
    "        self.botdir = None # 机器人当前方向\n",
    "        # 四个方向变换矩阵\n",
    "        # 用于计算机器人当前方向对应的前、左、右、后四个方向\n",
    "        self.alldir = numpy.array([[[1,0],[0,1]],\n",
    "            [[0,1],[-1,0]],[[0,-1],[1,0]],[[-1,0],[0,-1]]])\n",
    "        # 初始地图，0表示地面，2表示墙，1表示打扫过的地面\n",
    "        # 这是一个5*6的矩形房间\n",
    "        self.init_map = numpy.array([[2,2,2,2,2,2,2,2],[2,0,0,0,0,0,0,2],\n",
    "            [2,0,0,0,0,0,0,2],[2,0,0,0,0,0,0,2],[2,0,0,0,0,0,0,2],\n",
    "            [2,0,0,0,0,0,0,2],[2,2,2,2,2,2,2,2]])\n",
    "        # 调用reset方法将状态初始化，方法定义在后面列出\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        # 放置机器人的初始位置\n",
    "        self.pos = [2,2]\n",
    "        # 机器人的初始方向\n",
    "        self.botdir = [1,0]\n",
    "        # 地图的初始状态\n",
    "        self.map = numpy.copy(self.init_map)\n",
    "        # 设置机器人的初始电量略大于可打扫网格数量\n",
    "        self.power = self.map.shape[0] * self.map.shape[1]\n",
    "    def get_state(self):\n",
    "        # 计算机器人在当前位置和当前方向下\n",
    "        # 前、左、右、后四个方向相邻网格点坐标\n",
    "        allpos = numpy.matmul(self.botdir, self.alldir) + self.pos\n",
    "        # 取出这四个相邻网格的地图状态数值\n",
    "        allidx = numpy.ravel_multi_index(numpy.transpose(allpos), self.map.shape)\n",
    "        allval = numpy.take(self.map, allidx)\n",
    "        # 产生机器人当前的观察结果：是否是墙、是否打扫过\n",
    "        iswall = numpy.array(allval==2, dtype=numpy.float64)\n",
    "        isdone = numpy.array(allval==1, dtype=numpy.float64)\n",
    "        # 输出为张量\n",
    "        return torch.tensor([numpy.array([iswall, isdone]).flatten()], dtype=torch.float)\n",
    "    def do_action(self, action):\n",
    "        # 如果电量耗尽，给-100的惩罚，结束试验\n",
    "        if self.power <= 0:\n",
    "            return None, torch.tensor([-100], dtype=torch.float)\n",
    "        # 消耗一格电量\n",
    "        self.power -= 1\n",
    "        # 如果动作是左转(1)或者右转(2)\n",
    "        if action == 1 or action == 2:\n",
    "            # 更新机器人方向，给-1的能耗惩罚\n",
    "            self.botdir = numpy.matmul(self.botdir, self.alldir[action])\n",
    "            return self.get_state(), torch.tensor([-1], dtype=torch.float)\n",
    "        # 如果动作是前进(action == 0)\n",
    "        # 前进一个格子，更新机器人位置\n",
    "        self.pos = numpy.array(self.pos) + self.botdir\n",
    "        # 提取地图中当前格子的数值\n",
    "        posval = self.map[self.pos[0], self.pos[1]]\n",
    "        # 如果撞到了墙，给-100的惩罚，结束试验\n",
    "        if posval == 2:\n",
    "            return None, torch.tensor([-100], dtype=torch.float)\n",
    "        # 计算新的状态\n",
    "        next_state = self.get_state()\n",
    "        # 如果当前格子没有打扫过，将它标记为已经打扫过\n",
    "        if posval == 0:\n",
    "            self.map[self.pos[0], self.pos[1]] = 1\n",
    "            # 如果房间打扫完成，给100奖励，结束试验\n",
    "            if numpy.all(self.map > 0):\n",
    "                return None, torch.tensor([100], dtype=torch.float)\n",
    "            # 如果还没有打扫完成，给1个奖励，继续试验\n",
    "            return next_state, torch.tensor([1], dtype=torch.float)\n",
    "        # 如果当前格子已经打扫过，给-1的能耗惩罚，继续试验\n",
    "        return next_state, torch.tensor([-1], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abab4076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 值网络模型\n",
    "class DqnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DqnModel, self).__init__()\n",
    "        self.lin1 = nn.Linear(8, 32)\n",
    "        self.lin2 = nn.Linear(32, 3)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        return self.lin2(x)\n",
    "\n",
    "# %%\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "# 回放记忆的内容为状态转换\n",
    "# 用一个命名元组表示状态转换\n",
    "# 分别是当前状态、动作、下一个状态、奖励\n",
    "Transition = namedtuple('Transition',\n",
    "    ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        # 回放记忆的容量\n",
    "        self.capacity = capacity\n",
    "        # 初始化空的记忆\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    def push(self, *args):\n",
    "        # 记录一个状态转换\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    def sample(self, batch_size):\n",
    "        # 随机采样一批状态转换用作训练样本\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def __len__(self):\n",
    "        # 返回记忆的长度\n",
    "        return len(self.memory)\n",
    "\n",
    "#%%\n",
    "import math\n",
    "from itertools import count\n",
    "import torch.optim as optim\n",
    "\n",
    "class DqnTrainer(object):\n",
    "    def __init__(self):\n",
    "        self.BATCH_SIZE = 8 # 训练批量大小\n",
    "        self.GAMMA = 0.999 # 回报折扣因子\n",
    "        self.EPS_START = 0.95 # 探索策略的初始随机度\n",
    "        self.EPS_END = 0.5 # 探索策略的最终随机度\n",
    "        self.EPS_DECAY = 200 # 探索策略的随机度下降因子\n",
    "        self.policy_net = DqnModel() # 探索策略所采用的网络\n",
    "        self.target_net = DqnModel() # 最终训练的目标网络\n",
    "        # 同步目标网络和探索策略的状态\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        # 初始化优化器\n",
    "        self.optimizer = optim.RMSprop(self.policy_net.parameters())\n",
    "        # 初始化回放记忆\n",
    "        self.memory = ReplayMemory(10000)\n",
    "        self.steps_done = 0 # 记录试验次数\n",
    "        # 初始化环境\n",
    "        self.botenv = BotEnv()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        # 计算当前的探索策略随机度\n",
    "        eps_threshold = self.EPS_END+(self.EPS_START-self.EPS_END)*math.exp(\n",
    "            -1.0*self.steps_done/self.EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        # 当随机值大于随机度时，采用网络的最大输出决定采取的动作\n",
    "        # 否则，采取一个随机动作\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1,1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.botenv.n_actions)]], dtype=torch.long)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "        # 从回放记忆中进行采样\n",
    "        transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        # 用值网络预测每个转换中的状态和动作对应的期望回报值\n",
    "        # 这一步计算出的是网络的预测值\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        # 下面计算动作期望回报的“真实值”\n",
    "        # 这个真实值是采用时间差分法的近似\n",
    "        # 是基于下一个状态的值函数进行估计得到的\n",
    "        # 如果没有下一个状态，那么其对应的期望回报值为0\n",
    "        next_state_values = torch.zeros(self.BATCH_SIZE)\n",
    "        # 如果有下一个状态，用目标值网络计算状态动作值函数\n",
    "        # 选取下一个状态的最大回报动作对应的值作为状态的期望回报\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "            batch.next_state)), dtype=torch.bool)\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "        # 当前动作的预期回报的“真实值”可以用当前回报加上下一个状态的期望回报进行估计\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "        # 用当前动作回报的预测值和“真实值”计算网络的误差\n",
    "        # 这里采用平滑的绝对误差\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train_episode(self):\n",
    "        # 重置环境，开始一轮新试验\n",
    "        self.botenv.reset()\n",
    "        # 获取初始状态\n",
    "        state = self.botenv.get_state()\n",
    "        for _ in count():\n",
    "            # 执行一个动作\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward = self.botenv.do_action(action.item())\n",
    "            reward = torch.tensor([reward])\n",
    "            # 将状态转换记入回放记忆\n",
    "            self.memory.push(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "            # 用回放记忆中的数据训练模型\n",
    "            self.optimize_model()\n",
    "            # 如果试验结束，跳出循环\n",
    "            if state is None:\n",
    "                break\n",
    "        # 将优化过的探索策略网络同步到目标网络\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f346710",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DqnTrainer()\n",
    "# 进行10次迭代训练\n",
    "for i in range(10):\n",
    "    dqn.train_episode()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
